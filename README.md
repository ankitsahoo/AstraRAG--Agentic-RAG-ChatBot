# AstraRAG--Agentic-RAG-ChatBot
- AstraRAG â€” Agentic RAG Chatbot is an intelligent Retrieval-Augmented Generation (RAG) system powered by autonomous agents and LLMs to provide contextually aware, multi-step reasoning answers.
- It combines CrewAI, LlamaIndex, and Groq LLMs to create a chatbot that can understand queries, retrieve relevant knowledge, and generate high-quality responses based on your custom document data.
- Unlike standard chatbots, AstraRAG leverages Agentic workflows â€” allowing multiple agents to collaborate, plan, and reason dynamically before producing an answer.
- Built with FastAPI for backend orchestration and Streamlit for an interactive UI, the system is fully containerized using Docker and ready for cloud deployment on AWS EC2.

# âœ¨Key Highlights

- ğŸ§© Agentic Intelligence: Powered by CrewAI agents for reasoning and decision-making.
- ğŸ” RAG Pipeline: Uses LlamaIndex + ChromaDB for context-based retrieval from your knowledge base.
- âš¡ High-speed LLMs: Integrated with Groq API running Llama 3.3 70B for fast inference.
- ğŸ–¥ï¸ Modern Stack: FastAPI backend + Streamlit frontend for a seamless developer & user experience.
- â˜ï¸ Scalable Deployment: Fully containerized with Docker, deployable on AWS EC2 or any cloud service.

# Tech Stack I used
- **CrewAI**- will be powering my agents enabling mutlistep aware and context reasoning.
- **FastAPI**- Used in backend will send request and orchestrate the workflow.
- **StreamLit**- used as User Interface
- **Groq**- LLM Provider 
-**Model Used**- llama3.3:70b (42gb)
- **ChromaDB**- Used as vectorstore to manage embeddings and retreival.
- **LlamaIndex**- It will provide the RAG pipline to connect chabot with external Knowledge.
- **Docker**- To containrize the application
- **AWS EC2**- For Deployment

# flowchart

- ğŸ§‘User interacts with ğŸŒStreamlit UI and submits a query.
- âš¡FastAPI receives it and forwards the task to a ğŸ§ CrewAI Agent.
- The CrewAI Agent triggers the RAG Tool powered by ğŸ“š LlamaIndex and ğŸ—„ï¸ChromaDB.
- The RAG pipeline retrieves relevant documents and sends them with the query to Groq (ğŸ¤–Llama 3.3 70B) for -reasoning.
- The LLM generates a grounded response, which flows back to the frontend via FastAPI.
- The app is containerized with Docker and deployed on AWS EC2 for production access.

Hereâ€™s the query-response workflow:

| Step | Component                                | Action                                                                                     |
| ---- | ---------------------------------------- | ------------------------------------------------------------------------------------------ |
| 1    | ğŸ§‘ User                                   | Enters query â†’                                                                              |
| 2    | ğŸŒ Streamlit UI                           | Sends request to backend â†’                                                                  |
| 3    | âš¡ FastAPI Backend                         | Orchestrates workflow and forwards to CrewAI â†’                                              |
| 4    | ğŸ§  CrewAI Agent                           | Performs reasoning and multi-step task execution â†’                                         |
| 5    | ğŸ“š RAG Tool (LlamaIndex + ğŸ—„ï¸ ChromaDB)  | Retrieves relevant documents from vector store â†’                                           |
| 6    | ğŸ¤– Groq LLM API (Llama 3.3 70B)          | Generates grounded, context-aware answer â†’                                                 |
| 7    | ğŸ”„ Response Flow                          | Answer returned: RAG Tool â†’ CrewAI â†’ FastAPI â†’ Streamlit â†’                                  |
| 8    | ğŸŒ Streamlit UI                           | Displays the final answer to the user                                                      |
                                                   |

# âš™ï¸AstraRAG Complete Workflow Overview

### Query & Response Workflow

ğŸ§‘ **User**  
&nbsp;&nbsp;â†“  
ğŸŒ **Streamlit UI** â€“ Sends request to backend  
&nbsp;&nbsp;â†“  
âš¡ **FastAPI Backend** â€“ Orchestrates workflow and forwards to CrewAI  
&nbsp;&nbsp;â†“  
ğŸ§  **CrewAI Agent** â€“ Performs multi-step reasoning  
&nbsp;&nbsp;â†“  
ğŸ“š **RAG Tool (LlamaIndex + ğŸ—„ï¸ ChromaDB)** â€“ Retrieves relevant documents  
&nbsp;&nbsp;â†“  
ğŸ¤– **Groq LLM API (Llama 3.3 70B)** â€“ Generates context-aware answer  
&nbsp;&nbsp;â†‘  
ğŸ”„ **Response Flow** â€“ Answer returned: RAG â†’ CrewAI â†’ FastAPI â†’ Streamlit UI  
&nbsp;&nbsp;â†“  
ğŸŒ **Streamlit UI** â€“ Displays final answer

# ğŸ§± Deployment Flow

         +---------------------------------------------------------+
         | ğŸ–¥ï¸ App Components: ğŸŒ Streamlit + âš¡ FastAPI + ğŸ§  CrewAI  |
         |             + ğŸ“š LlamaIndex + ğŸ—„ï¸ ChromaDB               |
         +---------------------------------------------------------+
                              â”‚
                              â–¼
         +-------------------------------+
         | ğŸ³ Docker Container           |
         | (Containerized environment)  |
         +-------------------------------+
                              â”‚
                              â–¼
         +-------------------------------+
         | â˜ï¸ AWS EC2 Instance           |
         | (Production deployment)      |
         +-------------------------------+
                              â”‚
                              â–¼
         +-------------------------------+
         | ğŸŒ Public Access               |
         | (Users interact with chatbot) |
         +-------------------------------+


##### Some Screenshot related to Agentic RAG how it is retriving the answers to the Users query #######

**VectorDB creation**
![alt text](<Screenshot (29).png>)

**Agentic RAG Response**
![alt text](<Screenshot (30).png>) 
![alt text](<Screenshot (31).png>) 
![alt text](<Screenshot (32).png>)

# After integrating both frontend and backend 

**Frontend**
![alt text](<Screenshot (34).png>)

**In backend how agent is executing**
![alt text](<Screenshot (33).png>)

![alt text](<Screenshot (36).png>)

# Deployment ScreenShots

**Build the Docker Image of the AstraRAG**

# ğŸ³ Docker Image Details

| **Repository**     | **Tag**  | **Image ID**   | **Created**       | **Size**  |
|--------------------|----------|----------------|-------------------|-----------|
| astrarag-chatbot   | latest   | 34186c7d8610   | 14 minutes ago    | 13.7 GB   |

![alt text](<Screenshot (37).png>)

# âš™ï¸ Deployed the application to AWS EC2

![alt text](<Screenshot (38).png>)


